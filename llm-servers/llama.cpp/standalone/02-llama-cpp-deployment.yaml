apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-cpp-deployment
  labels:
    app: llama-cpp
    app.kubernetes.io/part-of: llama-cpp
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llama-cpp
  template:
    metadata:
      labels:
        app: llama-cpp
    spec:
      volumes:
        # - name: llama-cpp-pvc
        #   emptyDir:
        #     medium: Memory
        - name: llama-cpp-pvc
          persistentVolumeClaim:
            claimName: llama-cpp-pvc
      initContainers:
        - name: fetch-model-data
          image: ubi8
          volumeMounts:
            - name: llama-cpp-pvc
              mountPath: /models
          command:
            - sh
            - '-c'
            - |
              if [ ! -f /models/mistral-7b-instruct-v0.3.Q4_K_M.gguf ] ; then
                curl -L https://huggingface.co/SanctumAI/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/mistral-7b-instruct-v0.3.Q4_K_M.gguf --output /models/mistral-7b-instruct-v0.3.Q4_K_M.gguf
              else
                echo "model /models/mistral-7b-instruct-v0.3.Q4_K_M.gguf already present"
              fi
          resources:
            limits:
              cpu: '3'
              memory: 5Gi
            requests:
              cpu: '2'
              memory: 2Gi
      containers:
        - name: llama-cpp
          image: quay.io/milstein/llama-runtime-ubi:latest  # You can build and use your own image using provided Containerfile
          imagePullPolicy: IfNotPresent
          args: ["-m", "/models/mistral-7b-instruct-v0.3.Q4_K_M.gguf", "--prio", "3", "-c", "4096", "-b", "32", "-t", "48", "-n", "-1"]
          ports:
            - name: http
              protocol: TCP
              containerPort: 8080
          resources:
            limits:
              cpu: '5'
              memory: 12Gi
              nvidia.com/gpu: '1'
            requests:
              cpu: '4'
              memory: 10Gi
              nvidia.com/gpu: '1'
          livenessProbe:
            httpGet:
              path: /
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 20
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 3
          volumeMounts:
            - name: llama-cpp-pvc
              mountPath: /models
      restartPolicy: Always
      tolerations:
        - key: nvidia.com/gpu.product
          operator: Equal
          value: Tesla-V100-PCIE-32GB
          effect: NoSchedule
      nodeSelector:
        nvidia.com/gpu.product: Tesla-V100-PCIE-32GB
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
