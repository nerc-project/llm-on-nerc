{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "165a3407",
   "metadata": {},
   "source": [
    "## Querying a Milvus index - Nomic AI Embeddings\n",
    "\n",
    "Simple example on how to query content from a Milvus VectorStore. In this example, the embeddings are the fully open source ones released by NomicAI, [nomic-embed-text-v1](https://huggingface.co/nomic-ai/nomic-embed-text-v1).\n",
    "\n",
    "As described in [this blog post](https://blog.nomic.ai/posts/nomic-embed-text-v1), those embeddings feature a \"8192 context-length that outperforms OpenAI Ada-002 and text-embedding-3-small on both short and long context tasks\". In additions, they are:\n",
    "\n",
    "- Open source\n",
    "- Open data\n",
    "- Open training code\n",
    "- Fully reproducible and auditable\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- A **Milvus** vector database, set up according to [these instructions](../../../vector-databases/milvus/README.md).\n",
    "\n",
    "- Connection credentials to Milvus must be available as environment variables:\n",
    "\n",
    "  - `MILVUS_USERNAME`\n",
    "\n",
    "  - `MILVUS_PASSWORD`\n",
    "\n",
    "- Update the **MILVUS_HOST**, **MILVUS_PORT**, and **MILVUS_COLLECTION** in this notebook to match your deployment settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ac3132-6929-4477-9585-31761d7d9848",
   "metadata": {},
   "source": [
    "### Needed packages and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ed97389-9c5b-46a8-bedf-f28bf7038a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q einops==0.7.0 langchain==0.1.9 pymilvus==2.3.6 sentence-transformers==2.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c53e798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/pymilvus/client/__init__.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pymilvus import connections, Collection, CollectionSchema, FieldSchema, DataType, utility\n",
    "from langchain.vectorstores import Milvus\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b8ecae",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Base parameters, the Milvus connection info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9376e567",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace values according to your Milvus deployment\n",
    "MILVUS_HOST = \"milvus-service\"\n",
    "# MILVUS_HOST = \"milvus-service.<your-namespace>.svc.cluster.local\"\n",
    "MILVUS_PORT = 19530\n",
    "MILVUS_USERNAME = os.getenv('MILVUS_USERNAME')\n",
    "MILVUS_PASSWORD = os.getenv('MILVUS_PASSWORD')\n",
    "MILVUS_COLLECTION = \"collection_nomicai_embeddings\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d51868",
   "metadata": {},
   "source": [
    "### Initialize the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbb6a3e3-5ccd-441e-b80d-427555d9e9f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 2.4.0.dev0, however, your version is 2.4.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n",
      "/opt/app-root/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n",
      "2025-05-31 03:00:28.931079: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-31 03:00:29.912569: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Milvus vector store is ready!\n"
     ]
    }
   ],
   "source": [
    "# If you want to use a GPU, you can add the 'device': 'cuda' argument and provided you have used GPU Accelerator on your Workbench.\n",
    "# model_kwargs = {'trust_remote_code': True, 'device': 'cuda'}\n",
    "model_kwargs = {'trust_remote_code': True}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"nomic-ai/nomic-embed-text-v1\",\n",
    "    model_kwargs=model_kwargs,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\"host\": MILVUS_HOST, \"port\": MILVUS_PORT, \"user\": MILVUS_USERNAME, \"password\": MILVUS_PASSWORD},\n",
    "    collection_name=MILVUS_COLLECTION,\n",
    "    metadata_field=\"metadata\",\n",
    "    text_field=\"page_content\",\n",
    "    drop_old=False\n",
    "    )\n",
    "\n",
    "print(\"Milvus vector store is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9856851c",
   "metadata": {},
   "source": [
    "### Make a query to the index to verify sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9621e231-3541-40bc-85ef-8aa3b2ba2331",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec7c1dfc591344699dba3c58930b23f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2.20/html-single/working_on_data_science_projects/index\n",
      "https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2.20/html-single/getting_started_with_red_hat_openshift_ai_self-managed/index\n",
      "https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2.20/html-single/getting_started_with_red_hat_openshift_ai_self-managed/index\n",
      "https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2.20/html-single/openshift_ai_tutorial_-_fraud_detection_example/index\n"
     ]
    }
   ],
   "source": [
    "query=\"How can I create a Data Science Project?\"\n",
    "results = store.similarity_search_with_score(query, k=4, return_metadata=True)\n",
    "for result in results:\n",
    "    print(result[0].metadata['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1005e2c",
   "metadata": {},
   "source": [
    "### Work with a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "566f9347-a40a-4eeb-a690-e199b91947a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c378fbd-395d-43af-8cca-268bc05d0f51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94e4ab61ba6450d82fd99b3224e342d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='CHAPTER 1. USING DATA SCIENCE PROJECTS\\n1.1. CREATING A DATA SCIENCE PROJECT\\nTo implement a data science workflow, you must create a project. In OpenShift, a project is a Kubernetes\\nnamespace with additional annotations, and is the main way that you can manage user access to\\nresources. A project organizes your data science work in one place and also allows you to collaborate\\nwith other developers and data scientists in your organization.\\nWithin a project, you can add the following functionality:\\nConnections so that you can access data without having to hardcode information like endpoints\\nor credentials.\\nWorkbenches for working with and processing data, and for developing models.\\nDeployed models so that you can test them and then integrate them into intelligent\\napplications. Deploying a model makes it available as a service that you can access by using an\\nAPI.\\nPipelines for automating your ML workflow.\\nPrerequisites\\nYou have logged in to Red Hat OpenShift AI.', metadata={'source': 'https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2.20/html-single/working_on_data_science_projects/index', 'page': 7}),\n",
       " Document(page_content='and it must start with a letter and end with a letter or number.\\nNote:\\n You cannot change the resource name after the project is created. You can edit only the\\ndisplay name and the description.\\n5\\n. \\nOptional: In the \\nDescription\\n field, provide a project description.\\n6\\n. \\nClick \\nCreate\\n.\\nVerification\\nCHAPTER 3. CREATING A DATA SCIENCE PROJECT\\n7', metadata={'source': 'https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2.20/html-single/getting_started_with_red_hat_openshift_ai_self-managed/index', 'page': 10}),\n",
       " Document(page_content='CHAPTER 3. CREATING A DATA SCIENCE PROJECT\\nTo implement a data science workflow, you must create a project. In OpenShift, a project is a Kubernetes\\nnamespace with additional annotations, and is the main way that you can manage user access to\\nresources. A project organizes your data science work in one place and also allows you to collaborate\\nwith other developers and data scientists in your organization.\\nWithin a project, you can add the following functionality:\\nConnections so that you can access data without having to hardcode information like endpoints\\nor credentials.\\nWorkbenches for working with and processing data, and for developing models.\\nDeployed models so that you can test them and then integrate them into intelligent\\napplications. Deploying a model makes it available as a service that you can access by using an\\nAPI.\\nPipelines for automating your ML workflow.\\nPrerequisites\\nYou have logged in to Red Hat OpenShift AI.', metadata={'source': 'https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2.20/html-single/getting_started_with_red_hat_openshift_ai_self-managed/index', 'page': 10}),\n",
       " Document(page_content='The OpenShift AI dashboard shows the \\nHome\\n page.\\nNOTE\\nYou can navigate back to the OpenShift console by clicking the application launcher to\\naccess the OpenShift console.\\nFor now, stay in the OpenShift AI dashboard.\\nNext step\\nSetting up your data science project\\n2.2. SETTING UP YOUR DATA SCIENCE PROJECT\\nTo implement a data science workflow, you must create a data science project (as described in the\\nfollowing procedure). Projects allow you and your team to organize and collaborate on resources within\\nseparated namespaces. From a project you can create multiple workbenches, each with their own IDE\\nenvironment (for example, JupyterLab), and each with their own connections and cluster storage. In\\naddition, the workbenches can share models and data with pipelines and model servers.\\nPrerequisites\\nBefore you begin, log in to \\nRed Hat OpenShift AI\\n.\\nProcedure\\n1\\n. \\nOn the navigation menu, select \\nData science projects\\n. This page lists any existing projects that', metadata={'source': 'https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2.20/html-single/openshift_ai_tutorial_-_fraud_detection_example/index', 'page': 8})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(query)\n",
    "docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
